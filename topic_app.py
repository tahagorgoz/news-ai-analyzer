#######################################################################
#   TÃœRKÃ‡E HABER GÃœNDEM ANALÄ°ZÄ° â€” FULL VERSION (NO ERRORS)
#   AUTO INSTALL + BERTopic + BERT NER + 33 RSS + HEADLINE LIST
#######################################################################

import subprocess
import sys
import importlib
import feedparser
import re
import nltk
from nltk.corpus import stopwords


#######################################################################
#                     AUTO INSTALLER
#######################################################################

def auto_install(package):
    try:
        importlib.import_module(package)
        print(f"[OK] {package} zaten yÃ¼klÃ¼.")
    except ImportError:
        print(f"[INSTALL] {package} yÃ¼kleniyor...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])


REQUIRED_PACKAGES = [
    "torch",
    "transformers",
    "sentence-transformers",
    "bertopic",
    "feedparser",
    "nltk"
]

for pkg in REQUIRED_PACKAGES:
    auto_install(pkg)

print("\n[+] TÃ¼m paketler hazÄ±r!")


#######################################################################
#                     STOPWORDS
#######################################################################

nltk.download("stopwords")
stop_words = set(stopwords.words("turkish"))


#######################################################################
#                 BERT TABANLI TÃœRKÃ‡E NER
#######################################################################

print("[+] TÃ¼rkÃ§e BERT NER modeli yÃ¼kleniyor...")

from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

tokenizer_ner = AutoTokenizer.from_pretrained("savasy/bert-base-turkish-ner-cased")
model_ner = AutoModelForTokenClassification.from_pretrained("savasy/bert-base-turkish-ner-cased")

ner_pipeline = pipeline("ner", model=model_ner, tokenizer=tokenizer_ner, aggregation_strategy="simple")

print("[+] NER modeli hazÄ±r!")


#######################################################################
#                     HABER KAYNAKLARI (33 ADET)
#######################################################################

RSS_FEEDS = {

    # UluslararasÄ±
    "AA": "https://www.aa.com.tr/tr/rss/default?cat=guncel",
    "Reuters": "https://feeds.reuters.com/Reuters/worldNews",
    "BBC": "http://feeds.bbci.co.uk/news/world/rss.xml",
    "CNN International": "http://rss.cnn.com/rss/edition.rss",
    "Al Jazeera": "https://www.aljazeera.com/xml/rss/all.xml",
    "NY Times": "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
    "Guardian": "https://www.theguardian.com/world/rss",
    "Washington Post": "http://feeds.washingtonpost.com/rss/world",
    "AP News": "https://apnews.com/rss",
    "DW": "https://rss.dw.com/xml/feed/rss-en-all",
    "Euronews": "https://www.euronews.com/api/rss/most-read",

    # Ulusal
    "HÃ¼rriyet": "https://www.hurriyet.com.tr/rss/gundem",
    "Sabah": "https://www.sabah.com.tr/rss/gundem.xml",
    "Milliyet": "https://www.milliyet.com.tr/rss/rssNew/gundem.xml",
    "NTV": "https://www.ntv.com.tr/gundem.rss",
    "CNN TÃ¼rk": "https://www.cnnturk.com/feed/rss/all/news",
    "SÃ¶zcÃ¼": "https://www.sozcu.com.tr/rss/gundem.xml",
    "TRT Haber": "https://www.trthaber.com/rss/gundem.rss",
    "HabertÃ¼rk": "https://www.haberturk.com/rss",
    "Yeni Åafak": "https://www.yenisafak.com/rss?xml=gundem",
    "Cumhuriyet": "https://www.cumhuriyet.com.tr/rss/gundem.xml",
    "T24": "https://t24.com.tr/rss",
    "Diken": "https://www.diken.com.tr/feed/",
    "OdaTV": "https://odatv4.com/rss.php",
    "Ensonhaber": "https://www.ensonhaber.com/rss/ensonhaber.xml",
    "A Haber": "https://www.ahaber.com.tr/rss/anasayfa.xml",
    "Haber7": "https://www.haber7.com/rss/haber",
    "Karar": "https://www.karar.com/rss/haber",
    "BirGÃ¼n": "https://www.birgun.net/rss",
    "AkÅŸam": "https://www.aksam.com.tr/rss/haber",
    "Star": "https://www.star.com.tr/rss/rss.asp?cid=1",
    "Milli Gazete": "https://www.milligazete.com.tr/rss",
    "Evrensel": "https://www.evrensel.net/rss/haber.xml"
}

INTERNATIONAL = list(RSS_FEEDS.keys())[:11]
NATIONAL = list(RSS_FEEDS.keys())[11:]


#######################################################################
#                     HABER Ã‡EKME
#######################################################################

def fetch_rss(url):
    try:
        feed = feedparser.parse(url)
        return [
            (getattr(e, "title", "") + " " + getattr(e, "summary", "")).strip()
            for e in feed.entries
        ]
    except:
        return []


#######################################################################
#                     METÄ°N TEMÄ°ZLEME
#######################################################################

def clean_text(t):
    t = t.lower()
    t = re.sub(r"[^a-zÄŸÃ¼ÅŸÃ¶Ã§Ä±0-9 ]", " ", t)
    return " ".join([w for w in t.split() if w not in stop_words])


#######################################################################
#                     NER Ã‡IKARIMI
#######################################################################

def extract_entities(text):
    ent_raw = ner_pipeline(text)
    out = {"PER": [], "LOC": [], "ORG": [], "DATE": [], "OTHER": []}

    for item in ent_raw:
        tag = item["entity_group"]
        word = item["word"]

        if tag in out:
            out[tag].append(word)
        else:
            out["OTHER"].append(word)

    return out


#######################################################################
#                      BERTopic MODELÄ°
#######################################################################

print("[+] BERTopic iÃ§in embedding modeli yÃ¼kleniyor...")

from sentence_transformers import SentenceTransformer
embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", device="cpu")

from bertopic import BERTopic
topic_model = BERTopic(language="turkish", embedding_model=embedding_model)


#######################################################################
#                     TÃœM HABERLERÄ° Ã‡EK
#######################################################################

print("[+] Haberler toplanÄ±yor...")

all_texts = []
news_by_source = {}

for source, url in RSS_FEEDS.items():
    items = fetch_rss(url)
    if items:
        news_by_source[source] = items
        all_texts.extend(items)

print(f"[+] Toplam {len(all_texts)} haber alÄ±ndÄ±.")


#######################################################################
#                      TOPIC MODEL EÄÄ°T
#######################################################################

cleaned_all = [clean_text(t) for t in all_texts]

print("[+] BERTopic modeli eÄŸitiliyor (biraz sÃ¼rebilir)...")
topics, _ = topic_model.fit_transform(cleaned_all)


#######################################################################
#           TOPIC â†’ GENEL KATEGORÄ° EÅLEÅTÄ°RME
#######################################################################

def categorize(words):
    joined = " ".join(w[0] for w in words)

    rules = {
        "savas_jeopolitik": ["saldÄ±rÄ±","rusya","ukrayna","israil","iran","Ã§atÄ±ÅŸma","gaza"],
        "ekonomi": ["dolar","faiz","enflasyon","piyasa","kriz"],
        "siyaset": ["meclis","bakan","cumhurbaÅŸkanÄ±","seÃ§im","parti"],
        "asayis": ["tutuk","cinayet","mahkeme"],
        "saglik": ["hastane","kanser","salgÄ±n","tedavi"],
        "teknoloji": ["yapay","uzay","nasa","ai","robot"],
        "dogal_afet": ["deprem","sel","yangÄ±n"],
        "spor": ["maÃ§","transfer","lig","hakem"],
        "toplumsal": ["belediye","trafik","eÄŸitim","Ã¶ÄŸrenci"],
        "magazin": ["Ã¼nlÃ¼","dizi","film","oyuncu"]
    }

    for label, keys in rules.items():
        if any(k in joined for k in keys):
            return label

    return "diger"


#######################################################################
#         HER HABER SÄ°TESÄ° Ä°Ã‡Ä°N Ä°STATÄ°STÄ°K HESAPLA
#######################################################################

results = {}

for source, articles in news_by_source.items():

    cleaned_src = [clean_text(t) for t in articles]
    src_topics, _ = topic_model.transform(cleaned_src)

    counter = {k: 0 for k in [
        "savas_jeopolitik","ekonomi","siyaset",
        "asayis","saglik","teknoloji",
        "dogal_afet","spor","toplumsal","magazin","diger"
    ]}

    for t in src_topics:
        topic_words = topic_model.get_topic(t)
        cat = categorize(topic_words)
        counter[cat] += 1

    results[source] = counter

    print(f"\n=== {source} ===")
    total = sum(counter.values())
    for k,v in counter.items():
        if total > 0:
            print(f"{k}: %{round(v/total*100,2)}")


#######################################################################
#           ULUSAL VE ULUSLARARASI DAÄILIM
#######################################################################

def sum_stats(src_list):
    keys = list(next(iter(results.values())).keys())
    out = {k:0 for k in keys}

    for src in src_list:
        if src in results:
            for k,v in results[src].items():
                out[k] += v

    return out

intl = sum_stats(INTERNATIONAL)
nat  = sum_stats(NATIONAL)

print("\n=== ULUSLARARASI TOPLAM ===")
print(intl)

print("\n=== ULUSAL TOPLAM ===")
print(nat)


#######################################################################
#               NER BÃœYÃœK VARLIK ANALÄ°ZÄ°
#######################################################################

print("\n[+] Haberlerde geÃ§en kiÅŸi/ÅŸehir/Ã¼lke analiz ediliyor...")

entities = {"PER": [], "LOC": [], "ORG": [], "DATE": [], "OTHER": []}

for t in all_texts[:400]:
    e = extract_entities(t)
    for k,v in e.items():
        entities[k].extend(v)


#######################################################################
#               YORUM OLUÅTUR
#######################################################################

def gpt_summary():
    intl_main = max(intl, key=intl.get)
    nat_main = max(nat, key=nat.get)

    return f"""
=====================================================
               YAPAY ZEKA GÃœNDEM Ã–ZETÄ°
=====================================================

ğŸŒ ULUSLARARASI GÃœNDEMÄ°N Ã–NE Ã‡IKANI:
â†’ {intl_main}

ğŸ‡¹ğŸ‡· ULUSAL GÃœNDEMDE Ã–NE Ã‡IKAN:
â†’ {nat_main}

ğŸ“Œ Haberlerde en Ã§ok geÃ§en kiÅŸiler:
{entities['PER'][:10]}

ğŸ“Œ En Ã§ok geÃ§en ÅŸehir ve bÃ¶lgeler:
{entities['LOC'][:10]}

ğŸ“Œ En Ã§ok geÃ§en kurumlar:
{entities['ORG'][:10]}

=====================================================
"""

print(gpt_summary())


#######################################################################
#            HABER BAÅLIKLARI LÄ°STELEME SON SORU
#######################################################################

while True:
    choice = input("\nHaber baÅŸlÄ±klarÄ±nÄ± listelemek ister misiniz? (y/q): ").strip().lower()

    if choice == "q":
        print("\n[âœ”] Uygulama kapatÄ±ldÄ±.")
        break

    elif choice == "y":
        print("\n================ TÃœM HABER BAÅLIKLARI ================\n")

        for src, items in news_by_source.items():
            print(f"\n------------ {src} ------------\n")
            for i, t in enumerate(items, 1):
                print(f"{i}. {t}")

        print("\n======================================================")

    else:
        print("GeÃ§ersiz seÃ§im, tekrar deneyin.")
